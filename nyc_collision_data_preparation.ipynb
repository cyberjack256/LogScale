{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "> Before diving into the analysis, it's essential to clean and preprocess the data. In this section, we will:\n",
    "\n",
    "1. **Load the data** from the source file.\n",
    "\n",
    "2. **Inspect** the data for missing or incorrect values.\n",
    "\n",
    "3. **Clean** the data by handling missing or incorrect values, and standardize formats.\n",
    "\n",
    "4. **Transform** the data, if necessary, to create new features or better representations of existing features for our analysis.\n",
    "\n",
    "> By carefully preparing the data, we can ensure that our analysis is accurate and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock One))\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Two))\n",
    "\n",
    "#Load the NYPD Motor Collisions file\n",
    "df = pd.read_csv(\"/Users/Administrator/Documents/Motor_Vehicle_Collisions_-_Crashes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Three))\n",
    "\n",
    "df.info(verbose = True, show_counts = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyst Notes\n",
    "\n",
    "**Total records:** 1.9 million\n",
    "\n",
    "**Records with Borough and Zip Code info:** 1.3 million\n",
    "\n",
    "This means we're missing location records (Borough and Zip Code) for approximately 600k+ entries, or ~30% of the data.\n",
    "\n",
    "> **Note:** We could potentially use reverse geocoding with the `GeoPY` library to find the address (Borough and Zip Code) for records that have a location (Latitude and Longitude). However, this process is time-consuming and may not be feasible for this exercise. As a result, we won't be conducting reverse geocoding in our analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyst Notes\n",
    "## Renaming Fields for Consistency\n",
    "To ensure that our dataset aligns with our internal data model and follows a common structure, we will rename certain fields. This will make it easier to integrate our analysis with other data sources and facilitate collaboration across teams.\n",
    "\n",
    ">Follow these steps to rename fields:\n",
    "\n",
    "Review the current field names and compare them with the internal data model's requirements.\n",
    "Identify any discrepancies or inconsistencies in field names.\n",
    "Rename the fields to match the internal data model, ensuring that they adhere to a common naming convention.\n",
    "By standardizing field names, we can improve data quality and make our analysis more efficient and reliable.\n",
    "\n",
    "> **Note:** Remember to document any changes made to field names, as this will help maintain transparency and facilitate future data management tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Four))\n",
    "\n",
    "df.rename(columns = {  'CRASH DATE' : 'date',\n",
    "                       'CRASH TIME' : 'time',\n",
    "                       'COLLISION_ID' : 'collision_id',\n",
    "                       'BOROUGH' : 'borough',\n",
    "                       'ZIP CODE' : 'zip_code',\n",
    "                       'LATITUDE' : 'latitude',\n",
    "                       'LONGITUDE' : 'longitude',\n",
    "                       'LOCATION' : 'location',\n",
    "                       'ON STREET NAME'    : 'street_on',\n",
    "                       'CROSS STREET NAME' : 'street_cross',\n",
    "                       'OFF STREET NAME'   : 'street_off',\n",
    "                       'NUMBER OF PERSONS INJURED'     : 'total_injured',\n",
    "                       'NUMBER OF PERSONS KILLED'      : 'total_fatality',\n",
    "                       'NUMBER OF PEDESTRIANS INJURED' : 'ped_injured',\n",
    "                       'NUMBER OF PEDESTRIANS KILLED'  : 'ped_fatality',\n",
    "                       'NUMBER OF CYCLIST INJURED'     : 'cyc_injured',\n",
    "                       'NUMBER OF CYCLIST KILLED'      : 'cyc_fatality',\n",
    "                       'NUMBER OF MOTORIST INJURED'    : 'moto_injured',\n",
    "                       'NUMBER OF MOTORIST KILLED'     : 'moto_fatality',\n",
    "                       'CONTRIBUTING FACTOR VEHICLE 1' : 'veh_factor_1',\n",
    "                       'CONTRIBUTING FACTOR VEHICLE 2' : 'veh_factor_2',\n",
    "                       'CONTRIBUTING FACTOR VEHICLE 3' : 'veh_factor_3',\n",
    "                       'CONTRIBUTING FACTOR VEHICLE 4' : 'veh_factor_4',\n",
    "                       'CONTRIBUTING FACTOR VEHICLE 5' : 'veh_factor_5',\n",
    "                       'UNIQUE KEY' : 'unique_key',\n",
    "                       'VEHICLE TYPE CODE 1' : 'veh_type_1',\n",
    "                       'VEHICLE TYPE CODE 2' : 'veh_type_2',\n",
    "                       'VEHICLE TYPE CODE 3' : 'veh_type_3',\n",
    "                       'VEHICLE TYPE CODE 4' : 'veh_type_4',\n",
    "                       'VEHICLE TYPE CODE 5' : 'veh_type_5'},\n",
    "           inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Five))\n",
    "\n",
    "# Find all the keys with missing values and validate our data model changes\n",
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyst Notes\n",
    "\n",
    "> Assign missing Borough records to the value of 'NYC'\n",
    "\n",
    "Borough and Zip Code are missing for ~600k records, which is ~30% of the data. This is a significant portion, so we can't disregard it. We'll have 5 boroughs plus 'NYC' to tag the data with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Six))\n",
    "\n",
    "# Fill all blank values in column Borough\n",
    "# If a value is NaN it will be NYC\n",
    "df.loc[df['borough'].isnull(), 'borough'] = 'NYC'\n",
    "\n",
    "# View the output;`borough` should have 0 NaN values\n",
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyst Notes\n",
    " \n",
    "> Filter Out Total Injured and Total Fatality NaN values\n",
    "\n",
    "We should only keep records where the total number of injured and killed is greater than 0. This will ensure that we're focusing on relevant incidents in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Seven))\n",
    "\n",
    "# Remove NaN from TOTAL INJURED\n",
    "df = df.dropna(axis=0, subset=['total_injured'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Eight))\n",
    "\n",
    "# Remove NaN from TOTAL KILLED\n",
    "df = df.dropna(axis=0, subset=['total_fatality'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Nine))\n",
    "\n",
    "# If we are interested in collisions that have injuries, then we may  want to keep those values > 0 as df1\n",
    "df1 = df[(df['total_injured'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Ten))\n",
    "\n",
    "# If we are interested in the collisions that have fatalities, then we may want to keep those values > 0 as df2\n",
    "df2 = df[(df['total_fatality'] > 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Eleven))\n",
    "\n",
    "# To keep only those records with either injuries or fatalities we can  now concatenate df1 and df2 and put it back as df\n",
    "df = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Twelve))\n",
    "\n",
    "# Combine DATE and TIME column to transform Series to DateTime needed for further analysis\n",
    "df['date'] = df['date'] + ' ' + df['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Thirteen))\n",
    "\n",
    "# Convert string to DateTime\n",
    "df['date'] = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Fourteen))\n",
    "\n",
    "# Year filter\n",
    "df['year'] = pd.to_datetime(df['date']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Fifteen))\n",
    "\n",
    "# Quarter filter\n",
    "df['quarter'] = pd.to_datetime(df['date']).dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Sixteen))\n",
    "\n",
    "# Month filter\n",
    "df['month'] = pd.to_datetime(df['date']).dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Seventeen))\n",
    "\n",
    "# Day of the week filter\n",
    "df['weekday'] = pd.to_datetime(df['date']).dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Eighteen))\n",
    "\n",
    "#Fill in missing values with Empty\n",
    "df = df.fillna(value='EMPTY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Nineteenth))\n",
    "\n",
    "# Validate the final dataset before analysis\n",
    "df.info(verbose = True, show_counts = True)\n",
    "# We have ~430k relevant records instead of 1.9 million and our file is ~100 MB from ~400 MB at the beginning of preparation\n",
    "# This file is in a much better state to ingest into LogScale and begin working with a sample of the data.\n",
    "# Additionally, we have are steps to clean the data within LogScale via parser as ingest is streaming via API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((Codeblock Twenty))\n",
    "\n",
    "# Save the cleaned dataframe to a new CSV File\n",
    "df.to_csv(\"/Users/Administrator/Documents/clean_nyc_collisions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "log20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
